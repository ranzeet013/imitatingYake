{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4G29ErKeyuA",
        "outputId": "59dba63d-37f1-4930-a3ac-1a062ea4d9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.12.2)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ],
      "source": [
        "pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEfQE_-Ke7IH",
        "outputId": "32392c3e-7afa-415d-9c32-e781d5660906"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords_en = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize(document):\n",
        "    document = document.lower()\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = [token for token in tokenizer.tokenize(document) if token not in stopwords_en]\n",
        "    return tokens\n",
        "\n",
        "def tokenize_html_file(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        html_content = file.read()\n",
        "\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "    chapter = soup.find(\"div\", class_=\"chapter\", id=\"ch01\")\n",
        "\n",
        "    if chapter:\n",
        "        chapter_text = chapter.get_text(separator=\"\\n\", strip=True).lower()\n",
        "        return tokenize(chapter_text)\n",
        "    else:\n",
        "        print(\"Chapter not found in the HTML file.\")\n",
        "        return []\n",
        "\n",
        "def load_pickle_file(pickle_file_path):\n",
        "    try:\n",
        "        with open(pickle_file_path, \"rb\") as file:\n",
        "            data = pickle.load(file)\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Pickle file {pickle_file_path} not found.\")\n",
        "        return []\n",
        "\n",
        "def generate_candidates(tokens, n_max=3):\n",
        "    candidates = []\n",
        "    for n in range(1, n_max + 1):\n",
        "        ngram_candidates = list(ngrams(tokens, n))\n",
        "        candidates.extend([' '.join(ngram) for ngram in ngram_candidates])\n",
        "    return candidates\n",
        "\n",
        "def casing_feature(keyword):\n",
        "    return any(word[0].isupper() for word in keyword.split())\n",
        "\n",
        "def word_position_feature(keyword, tokens):\n",
        "    words = keyword.split()\n",
        "    first_word_pos = tokens.index(words[0]) if words[0] in tokens else -1\n",
        "    return 1 / (first_word_pos + 1) if first_word_pos != -1 else 0\n",
        "\n",
        "def word_frequency_feature(keyword, tokens):\n",
        "    count = tokens.count(keyword)\n",
        "    return count / len(tokens)\n",
        "\n",
        "def word_relatedness_feature(keyword, tokens, window_size=5):\n",
        "    words = keyword.split()\n",
        "    related_count = 0\n",
        "    for i, word in enumerate(tokens):\n",
        "        if word == words[0]:\n",
        "            window = tokens[i+1:i+1+window_size]\n",
        "            related_count += sum(1 for w in words[1:] if w in window)\n",
        "    return related_count\n",
        "\n",
        "def context_dispersion_feature(keyword, tokens, window_size=5):\n",
        "    words = keyword.split()\n",
        "    positions = [i for i, word in enumerate(tokens) if word == words[0]]\n",
        "    dispersion_score = 0\n",
        "    for pos in positions:\n",
        "        window = tokens[pos:pos+window_size]\n",
        "        dispersion_score += sum(1 for w in words[1:] if w in window)\n",
        "    return dispersion_score / len(positions) if positions else 0\n",
        "\n",
        "def score_candidates(candidates, tokens):\n",
        "    scored_candidates = []\n",
        "    for candidate in candidates:\n",
        "        f1 = casing_feature(candidate)\n",
        "        f2 = word_position_feature(candidate, tokens)\n",
        "        f3 = word_frequency_feature(candidate, tokens)\n",
        "        f4 = word_relatedness_feature(candidate, tokens)\n",
        "        f5 = context_dispersion_feature(candidate, tokens)\n",
        "\n",
        "        score = f1 + f2 + f3 + f4 + f5\n",
        "        scored_candidates.append((candidate, score))\n",
        "    return scored_candidates\n",
        "\n",
        "def rank_keywords_by_frequency(candidates, tokens):\n",
        "    frequency = Counter(tokens)\n",
        "    ranked_keywords = [(keyword, frequency[keyword]) for keyword in candidates if keyword in frequency]\n",
        "    ranked_keywords = sorted(ranked_keywords, key=lambda x: x[1], reverse=True)\n",
        "    return ranked_keywords\n",
        "\n",
        "# Inference code\n",
        "html_file_path = \"/content/pg75577-images.html\"\n",
        "pickle_file_path = \"/content/chapter_tokens.p\"\n",
        "\n",
        "html_tokens = tokenize_html_file(html_file_path)\n",
        "print(\"Sample HTML Tokens:\", html_tokens[:50])\n",
        "\n",
        "chapter_tokens = load_pickle_file(pickle_file_path)\n",
        "print(\"Sample Resume Tokens:\", chapter_tokens[:50])\n",
        "\n",
        "candidates = generate_candidates(html_tokens)\n",
        "scored_candidates = score_candidates(candidates, html_tokens)\n",
        "ranked_keywords = rank_keywords_by_frequency(candidates, html_tokens)\n",
        "\n",
        "print(\"Top 15 Scored Candidates:\", scored_candidates[:15])\n",
        "print(\"Top 15 Ranked Keywords:\", ranked_keywords[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwAlm8BlfJFP",
        "outputId": "a13809aa-3661-44f3-a72c-6b25742d6d6b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample HTML Tokens: ['chapter', 'heavy', 'glass', 'bronze', 'door', 'revolved', 'released', 'sections', 'grizzly', 'november', 'mist', 'rosy', 'fragrant', 'hotel', 'lobby', 'malice', 'envy', 'joy', 'enthusiasm', 'vanity', 'greed', 'fear', 'masked', 'dignity', 'wrapped', 'sealskin', 'topped', 'charming', 'bright', 'red', 'hat', 'came', 'quickly', 'alone', 'two', 'egg', 'shaped', 'matrons', 'glanced', 'lengthened', 'set', 'glances', 'purple', 'henna', 'breathed', 'beautiful', 'wrap', 'tell', 'minute', 'brown']\n",
            "Sample Resume Tokens: ['chapter', 'heavy', 'glass', 'bronze', 'door', 'revolved', 'released', 'sections', 'grizzly', 'november', 'mist', 'rosy', 'fragrant', 'hotel', 'lobby', 'malice', 'envy', 'joy', 'enthusiasm', 'vanity', 'greed', 'fear', 'masked', 'dignity', 'wrapped', 'sealskin', 'topped', 'charming', 'bright', 'red', 'hat', 'came', 'quickly', 'alone', 'two', 'egg', 'shaped', 'matrons', 'glanced', 'lengthened', 'set', 'glances', 'purple', 'henna', 'breathed', 'beautiful', 'wrap', 'tell', 'minute', 'brown']\n",
            "Top 15 Scored Candidates: [('chapter', 1.0003147623544224), ('heavy', 0.5006295247088448), ('glass', 0.3336480956877557), ('bronze', 0.2503147623544224), ('door', 0.20188857412653446), ('revolved', 0.16698142902108906), ('released', 0.14317190521156525), ('sections', 0.1253147623544224), ('grizzly', 0.11142587346553351), ('november', 0.10031476235442241), ('mist', 0.09122385326351332), ('rosy', 0.08364809568775573), ('fragrant', 0.07723783927749933), ('hotel', 0.07268762084626107), ('lobby', 0.06698142902108907)]\n",
            "Top 15 Ranked Keywords: [('neal', 64), ('neal', 64), ('neal', 64), ('neal', 64), ('neal', 64), ('neal', 64), ('neal', 64), ('neal', 64), ('neal', 64), ('neal', 64), ('neal', 64), ('neal', 64), ('neal', 64), ('neal', 64), ('neal', 64)]\n"
          ]
        }
      ]
    }
  ]
}